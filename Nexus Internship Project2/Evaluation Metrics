import spacy
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt


df = pd.read_csv('test.csv')

# Load spaCy's pre-trained model for text processing
nlp = spacy.load("en_core_web_sm")

# Split the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Define the pipeline with CountVectorizer and Logistic Regression
pipeline = Pipeline([
    ('vectorizer', CountVectorizer(analyzer='word', tokenizer=nlp.tokenizer, stop_words='english')),
    ('classifier', LogisticRegression())
])

# Fit the model on the training data
pipeline.fit(train_df['text'], train_df['sentiment'])

# Make predictions on the test data
predictions = pipeline.predict(test_df['text'])
proba_predictions = pipeline.predict_proba(test_df['text'])[:, 1]  # Probability estimates for positive class

# Evaluate the model
accuracy = accuracy_score(test_df['sentiment'], predictions)
classification_report_output = classification_report(test_df['sentiment'], predictions)
conf_matrix = confusion_matrix(test_df['sentiment'], predictions)

# Display basic evaluation metrics
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report_output)
print("Confusion Matrix:")
print(conf_matrix)

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(test_df['sentiment'], proba_predictions)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', label='Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# ROC Curve and AUC
fpr, tpr, _ = roc_curve(test_df['sentiment'], proba_predictions)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='orange', label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()
